# Results and Discussion

## Relationship between Model Performance and Calibration Sample Sizes

The model performance in terms of precision and recall is presented in Table 2 and Figure 3. With a calibration set consisting of only 64 images, similar in background to the test images, the model demonstrates reasonably good performance on subsets A01, A02, and A03. The average precision achieved is 87.96%, 76.01%, and 76.76%, respectively, while the average recall is 87.78%, 77.58%, and 70.23%, respectively. However, increasing the calibration set size does not consistently improve model performance. For instance, in subset A03, doubling the calibration size from 128 to 256 images results in only a slight improvement of 0.22% in precision and 0.41% in recall. Even when increasing the calibration size twentyfold, from 64 to 1024 images, the maximum gains observed are 7.54% in precision for subset A02 and 11.64% in recall for subset A03. In subset A01, these improvements are even smaller, at just 5.14% and 5.33%, respectively. When detection an image containing 20 ants, the 5% improvement in precision and recall corresponds to correctly detecting just one additional ant. These findings suggest the presence of a saturation point in model performance; when the detection task is relatively straightforward and the background is consistent, a small number of calibration images is sufficient to achieve good performance. 

In contrast, for more complex and diverse backgrounds, the relationship between sample size and model performance varies significantly between subsets B01 and B02. In subset B01, the increase in performance is almost linear as the calibration size doubles, with precision and recall improving by 14.64% and 13.09%, respectively. Notably, when all calibration images are utilized, the model’s performance on B01, despite the new and complex background, is nearly equivalent to that on subsets A02 and A03, which feature similar backgrounds. This suggests that deploying the CV system in a new environment requires a large calibration set to ensure the model can generalize effectively to novel images.

Interestingly, in subset B02, characterized by sparse ant distribution and uneven background colors, increasing the calibration size does not significantly enhance model performance. In fact, when the calibration size exceeds 512 images, precision actually decreases. This finding indicates that the model may overfit to the calibration set when the background is highly diverse, leading to generating more false detections and reducing precision.

When comparing manual and automated counting results (Figure 4) using a calibration size of 1024, all subsets except B02 show a high correlation between the automated and manual counts, with a minimum squared correlation (r²) of 0.94. For subsets with similar backgrounds, the RMSE values are 0.96, 1.55, and 1.23 for A01, A02, and A03, respectively. In subset B01, where the background includes multiple black stains resembling ants, the automated counts achieve an r² of 0.95 but with a higher RMSE of 11.36, indicating an overestimation by approximately 11 ants on average. This overestimation is primarily due to false detections caused by background stains that resemble ants.

The poorest performance is observed in subset B02, where the RMSE is 1.50 and r² drops to 0.58. This suggests that the sparse and small distribution of ants makes the counting results highly sensitive to false detections, significantly affecting the correlation. These findings highlight the importance of background uniformity and ant distribution in ensuring accurate automated counting results.

## Data dissemination and web-based application

The studied ant images with YOLO annotation is organized at 